# NC-RNN

Neural collapse (NC) describes a geometric convergence that emerges in the final epoch of training in feed-forward and convolutional networks: class means align with one-hot vertices, features collapse to centroids, and classifiers converge to equi-angular simplices. While NC has been documented for image classifiers, its presence in recurrent neural networks (RNNs) remains untested. This project asks: Does NC arise during the terminal phase of training (TPT) in sequence-classification RNNs?
